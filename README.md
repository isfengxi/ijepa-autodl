

# I-JEPA (AutoDL Adapted Version)

This repository is an AutoDL-adapted version of the official I-JEPA implementation from Meta AI.

Original repository:
ğŸ”— [https://github.com/facebookresearch/ijepa](https://github.com/facebookresearch/ijepa)

This version includes:

* AutoDL-compatible configs
* CSI dataset support
* Simplified single-GPU training
* Cleaner project structure for reproducibility

---

## ğŸ“‚ Project Structure

```text
ijepa/
â”œâ”€â”€ configs/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ datasets/
â”‚   â”œâ”€â”€ masks/
â”‚   â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ helper.py
â”‚   â”œâ”€â”€ train.py
â”‚   â””â”€â”€ transforms.py
â”œâ”€â”€ main.py
â”œâ”€â”€ main_distributed.py
â””â”€â”€ README.md
```

---

## ğŸ§  What is I-JEPA?

I-JEPA (Image Joint Embedding Predictive Architecture) is a self-supervised representation learning framework introduced by Meta AI.

Instead of reconstructing pixels (like MAE), I-JEPA predicts representations of masked image blocks in embedding space.

---

## âš™ï¸ Environment Setup (AutoDL)

### 1ï¸âƒ£ Create environment

```bash
conda create -n ijepa python=3.10 -y
conda activate ijepa
```

### 2ï¸âƒ£ Install dependencies

```bash
pip install torch torchvision timm numpy pyyaml
```

If using ImageNet training, make sure:

* torchvision supports ImageFolder
* CUDA is correctly configured

---

## ğŸ“Š Dataset Preparation

### CSI Dataset

Place dataset under:

```bash
/root/autodl-tmp/csi_preprocessed/
```

Then modify config:

```yaml
data:
  root_path: /root/autodl-tmp
```

---

### ImageNet (Optional)

Place ImageNet root under:

```bash
/root/autodl-tmp/ImageNet/
```

Ensure folder structure:

```text
ImageNet/
â”œâ”€â”€ train/
â”‚   â”œâ”€â”€ class1/
â”‚   â”œâ”€â”€ class2/
â”‚   â””â”€â”€ ...
â””â”€â”€ val/
```

---

## ğŸš€ Training

### ğŸ”¹ Single GPU

```bash
python -u main.py \
  --fname configs/csi_autodl.yaml \
  --devices cuda:0
```

---

### ğŸ”¹ Distributed Training

```bash
python -m torch.distributed.run \
  --nproc_per_node=2 \
  main_distributed.py \
  --fname configs/in1k_vith14_ep300_autodl.yaml
```

---

## ğŸ“ Output

Training outputs are saved to:

```yaml
logging:
  folder: /root/autodl-tmp/ijepa_runs/
```

Checkpoints and logs are automatically generated.

---

## ğŸ”¬ Differences from Official Repo

This version:

* Removes unnecessary distributed complexity for single-GPU
* Adds CSI dataset loader
* Provides AutoDL-ready configs
* Cleans git structure and ignores training artifacts

---

## ğŸ“Œ Notes

* Do not commit datasets or checkpoints.
* Ensure `.gitignore` excludes large files.
* Recommended batch size depends on GPU memory.

---

## ğŸ“œ License

This project follows the original license from Meta AI.

---

## ğŸ™Œ Acknowledgement

Based on:

Meta AI â€“ I-JEPA
[https://github.com/facebookresearch/ijepa](https://github.com/facebookresearch/ijepa)

---

# Optional: Add Experiment Tracking Section (Recommended)

If you want, we can add:

* Experiment logs table
* Model comparison (MAE vs I-JEPA)
* Pretraining + reconstruction explanation
* Citation section

---

å¦‚æœä½ æ„¿æ„ï¼Œæˆ‘å¯ä»¥ä¸‹ä¸€æ­¥å¸®ä½ ï¼š

* âœ… å†™ä¸€ä¸ª **æ›´å­¦æœ¯ç‰ˆæœ¬ READMEï¼ˆå«å…¬å¼è§£é‡Šï¼‰**
* âœ… åŠ ä¸€ä¸ª â€œMAE vs I-JEPAâ€ å¯¹æ¯”è¡¨
* âœ… å¢åŠ  Reproducibility checklist
* âœ… åŠ è‡ªåŠ¨è®­ç»ƒè„šæœ¬ç›®å½• `scripts/`
* âœ… è®©ä»“åº“è¾¾åˆ°â€œè®ºæ–‡çº§å¼€æºè´¨é‡â€

ä½ ç°åœ¨è¿™ä¸ªé¡¹ç›®ï¼Œå·²ç»å¯ä»¥å¾€â€œç§‘ç ”å¯å±•ç¤ºä»“åº“â€æ–¹å‘è¿›åŒ–äº†ã€‚
